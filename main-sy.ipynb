{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDRNN\n",
    "[Description]\n",
    "(Add description here)\n",
    "\n",
    "The goal of the project:\n",
    "1. palceholder\n",
    "2. placeholder\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First download the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import  Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data and print it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataloader import ProteinDataset, proteinread\n",
    "from dataloader import AminoAcidDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this path with the actual path to your input CSV file\n",
    "input_csv_file = \"data.csv\"\n",
    "\n",
    "# # Create an instance of the ProteinDataset\n",
    "# train_data, seq_length = proteinread(input_csv_file)\n",
    "\n",
    "# # Create dataset instances for training and validation\n",
    "# train_data = ProteinDataset(train_data)\n",
    "\n",
    "# # Create data loaders for training and validation\n",
    "# batch_size = 1  # You can adjust this batch size\n",
    "# train_dataset = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "max_sequence_length = 1000\n",
    "num_amino_acids = 20\n",
    "\n",
    "dataset = AminoAcidDataset(input_csv_file, max_sequence_length, num_amino_acids)\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 16\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "# for batch in dataloader:\n",
    "#     print(\"Batch shape:\", batch.shape)\n",
    "#     print(\"Batch data:\", batch)\n",
    "#     break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick up one sequence to show how our data looks like\n",
    "g, g_len, target = dataset[65]\n",
    "print(g)\n",
    "print(g_len)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an empty list to store the numerical sequences\n",
    "training_sequences = []\n",
    "# Iterate over the training_dataset\n",
    "for sample in dataset:\n",
    "    # Extract the numerical part (the first element of the tuple) and append it to the list\n",
    "    training_sequences.append(sample[0])\n",
    "\n",
    "# Print the list of training sequences\n",
    "print(training_sequences[65])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Help function"
    ]
   },
   "outputs": [],
   "source": [
    "print(training_sequences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function and calculation (kld reconstruct loss) and printer form tuorial 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Help function from tutroial 6 to calculate loss and plotting\n",
    "\n",
    "# def sample(mean, log_var):\n",
    "#     std = torch.exp(0.5*log_var)\n",
    "#     eps = torch.randn_like(std, device=mean.device)\n",
    "#     return mean + eps*std\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return total_params\n",
    "def plot_losses(reco_losses, kld_losses):\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.plot(reco_losses)\n",
    "    ax.set_title('Reconstruction Loss')\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    ax.plot(kld_losses)\n",
    "    ax.set_title('KLD Loss')\n",
    "# def generate_seq(decoder_nn, num_seq=6):\n",
    "#     new_z = torch.randn(num_seq, 3)\n",
    "#     if torch.cuda.is_available():\n",
    "#         new_z = new_z.cuda()\n",
    "\n",
    "#     new_pred = decoder_nn(new_z)\n",
    "\n",
    "#     new_pred = new_pred.cpu().detach().numpy()\n",
    "\n",
    "#     for i in range(32):\n",
    "#         ax = fig.add_subplot(gs[i])\n",
    "#         img_array = new_pred[i][0]\n",
    "#         ax.imshow(img_array, cmap='gray')\n",
    "#         ax.axis('off')\n",
    "#         if cond:\n",
    "#             ax.set_title(f'Cond input: {rand_int[i]}')\n",
    "def plot_z_dist(train_dl, encoder_nn, cond=False):\n",
    "    fig = plt.figure(figsize=(24, 6))\n",
    "    gs = GridSpec(1, 4)\n",
    "\n",
    "\n",
    "    mus = []; log_vars = []; zs = []; ys = []\n",
    "    for xb, yb, yb_one_hot in train_dl:\n",
    "        if torch.cuda.is_available():\n",
    "            xb = xb.cuda()\n",
    "            yb_one_hot = yb_one_hot.cuda()\n",
    "\n",
    "        if cond:\n",
    "            mu, log_var = encoder_nn(xb, yb_one_hot)\n",
    "        else:\n",
    "            mu, log_var = encoder_nn(xb)\n",
    "        z = sample(mu, log_var)\n",
    "\n",
    "        mu = mu.cpu().detach().numpy()\n",
    "        log_var = log_var.cpu().detach().numpy()\n",
    "        z = z.cpu().detach().numpy()\n",
    "\n",
    "        mus.extend(mu.tolist())\n",
    "        log_vars.extend(log_var.tolist())\n",
    "        zs.extend(z.tolist())\n",
    "        ys.extend(yb.tolist())\n",
    "\n",
    "    zs = np.array(zs)\n",
    "    mus = np.array(mus)\n",
    "    log_vars = np.array(log_vars)\n",
    "    ys = np.array(ys)\n",
    "\n",
    "\n",
    "    ax = fig.add_subplot(gs[0])\n",
    "    ax.scatter(zs[:, 0], zs[:, 1], c=ys, cmap='tab10', alpha=0.3, s=1)\n",
    "    ax.set_title('Z')\n",
    "    ax.grid()\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    gauss0 = np.random.normal(0, 1, len(zs[:, 0]))\n",
    "    gauss1 = np.random.normal(0, 1, len(zs[:, 0]))\n",
    "    ax = fig.add_subplot(gs[1])\n",
    "    ax.scatter(gauss0, gauss1, c='cornflowerblue', alpha=0.3, s=1)\n",
    "    ax.set_title('Gaus(0,1))')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    return ys, zs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trainning loop based on tutorial 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_VAE import VAEGPT, Vaeln\n",
    "\n",
    "latent_dim = 32\n",
    "\n",
    "# vae = VAEGPT(input_size=21, output_size=21, latent_dim=latent_dim)\n",
    "\n",
    "hid_layer = [1000, 500, 100, 32]\n",
    "vae = Vaeln(input_dim=21*1000, latent_dim=latent_dim, hid_layer=hid_layer)\n",
    "\n",
    "# 看我派出cuda把你们都送上天！\n",
    "if torch.cuda.is_available():\n",
    "    vae.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'VAE has {count_parameters(vae):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "beta = 1 # Weight of the KL divergence term\n",
    "\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    # Reconstruction loss using cross-entropy\n",
    "\n",
    "    cross_entropy_loss = F.cross_entropy(recon_x.permute(0, 2, 1), x.argmax(dim=2), reduction='mean')\n",
    "\n",
    "    # Regularization term (KL divergence)\n",
    "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    # Combine reconstruction loss and regularization term\n",
    "    total_loss = cross_entropy_loss + (beta * kl_divergence)\n",
    "    \n",
    "    return kl_divergence, total_loss\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    vae.train()\n",
    "    total_losses = []\n",
    "    # reco_losses = []\n",
    "    kld_losses  = []\n",
    "    for seq_batch, length_batch, target_batch in dataloader:\n",
    "        if torch.cuda.is_available():\n",
    "            seq_batch = seq_batch.cuda()\n",
    "            length_batch = length_batch.cuda()\n",
    "            target_batch = target_batch.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mean, logvar = vae(seq_batch)\n",
    "        kld_loss, total_loss = vae_loss(recon_batch, seq_batch, mean, logvar)\n",
    "        \n",
    "        total_loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(vae.parameters(), max_norm=10000)\n",
    "        optimizer.step()\n",
    "        total_losses.append(total_loss.cpu().detach().numpy())\n",
    "        # reco_losses.append(reco_loss.cpu().detach().numpy())\n",
    "        kld_losses.append(kld_loss.cpu().detach().numpy())\n",
    "        # total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_losses(reco_losses, kld_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to convert one-hot encoding to amino acid sequences\n",
    "def one_hot_to_sequence(one_hot):\n",
    "    _, max_indices = torch.max(one_hot, dim=1)\n",
    "    amino_acids = [\"A\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \"M\", \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"W\", \"Y\", \"\"]\n",
    "    sequences = []\n",
    "    for indices in max_indices:\n",
    "        sequence = \"\".join([amino_acids[idx] if idx < len(amino_acids) else '-' for idx in indices if idx != 0])\n",
    "        sequences.append(sequence)\n",
    "    return sequences\n",
    "\n",
    "# Test the trained model on an example from the training dataset\n",
    "test_batch_size = 1  # Set batch size for testing\n",
    "test_dataloader = DataLoader(dataset, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "# Retrieve an example from the test dataset\n",
    "example_seq, example_len, example_target = next(iter(test_dataloader))\n",
    "\n",
    "# Move the input tensor to the same device as the model\n",
    "device = next(vae.parameters()).device\n",
    "example_seq = example_seq.to(device)\n",
    "example_len = example_len.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "vae.eval()\n",
    "\n",
    "# Perform inference on the example\n",
    "with torch.no_grad():\n",
    "    recon_output, _, _ = vae(example_seq)\n",
    "    recon_output = torch.softmax(recon_output.permute(0,2,1), dim=1)\n",
    "\n",
    "# Convert the output to amino acid sequences\n",
    "amino_acid_sequences = one_hot_to_sequence(recon_output)\n",
    "\n",
    "# Print the reconstructed sequences\n",
    "for i, sequence in enumerate(amino_acid_sequences):\n",
    "    print(f\"Example {i+1}: {sequence}\")\n",
    "\n",
    "# Save the trained model as a .pt file\n",
    "model_state_dict = vae.state_dict()  # Get the model's state_dict\n",
    "device = next(vae.parameters()).device  # Get the device of the model\n",
    "\n",
    "# Move the model's state_dict to the CPU if it's currently on a GPU\n",
    "if device.type == 'cuda':\n",
    "    model_state_dict = {k: v.to('cpu') for k, v in model_state_dict.items()}\n",
    "\n",
    "# Save the model's state_dict to a .pt file\n",
    "torch.save(model_state_dict, \"trained_vae_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_input = one_hot_to_sequence(example_seq.transpose(1,2))\n",
    "for i, sequence in enumerate(original_input):\n",
    "    print(f\"Original {i+1}: {sequence}\")\n",
    "print(example_seq)\n",
    "print(recon_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
