{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDRNN\n",
    "[Description]\n",
    "(Add description here)\n",
    "\n",
    "The goal of the project:\n",
    "1. palceholder\n",
    "2. placeholder\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First download the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "from torch.utils.data import  Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data and print it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import ProteinDataset, proteinread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 490 at dim 1 (got 491)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m input_csv_file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdata.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[39m# Create an instance of the ProteinDataset\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m dataset \u001b[39m=\u001b[39m proteinread(input_csv_file)\n\u001b[0;32m      8\u001b[0m \u001b[39m# Define the validation set size (20% of the total data)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m training_size \u001b[39m=\u001b[39m \u001b[39m0.8\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\pukqq\\OneDrive - weizmann.ac.il\\Practical DL\\IDRNN\\dataloader.py:41\u001b[0m, in \u001b[0;36mproteinread\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     38\u001b[0m padded_binary_sequence_matrices \u001b[39m=\u001b[39m [seq \u001b[39m+\u001b[39m [[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m20\u001b[39m] \u001b[39m*\u001b[39m (max_sequence_length \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(seq)) \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m binary_sequence_matrices]\n\u001b[0;32m     40\u001b[0m \u001b[39m# Convert the padded matrices to a tensor\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m binary_sequence_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(padded_binary_sequence_matrices, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32)\n\u001b[0;32m     42\u001b[0m \u001b[39mreturn\u001b[39;00m binary_sequence_tensor\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 490 at dim 1 (got 491)"
     ]
    }
   ],
   "source": [
    "# Replace this path with the actual path to your input CSV file\n",
    "from sklearn.model_selection import train_test_split\n",
    "input_csv_file = \"data.csv\"\n",
    "\n",
    "# Create an instance of the ProteinDataset\n",
    "dataset = proteinread(input_csv_file)\n",
    "\n",
    "# Define the validation set size (20% of the total data)\n",
    "training_size = 0.8\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_data, val_data = train_test_split(dataset, train_size=training_size, random_state=42)\n",
    "\n",
    "# Create dataset instances for training and validation\n",
    "train_data, val_data = ProteinDataset(train_data), ProteinDataset(val_data)\n",
    "\n",
    "# Create data loaders for training and validation\n",
    "batch_size = 64  # You can adjust this batch size\n",
    "train_dataset = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick up one sequence to show how our data looks like\n",
    "g = training_dataset[65]\n",
    "\n",
    "print(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get training sequence and validation sequence only\n",
    "\n",
    "\n",
    "# Create an empty list to store the numerical sequences\n",
    "training_sequences = []\n",
    "validation_sequences = []\n",
    "# Iterate over the training_dataset and  validation dataset\n",
    "for sample in training_dataset:\n",
    "    # Extract the numerical part (the first element of the tuple) and append it to the list\n",
    "    training_sequences.append(sample[0])\n",
    "\n",
    "for sample in validation_dataset:\n",
    "    # Extract the numerical part (the first element of the tuple) and append it to the list\n",
    "    validation_sequences.append(sample[0])\n",
    "    \n",
    "# Print the list of training sequences\n",
    "print(training_sequences[65])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Help function"
    ]
   },
   "outputs": [],
   "source": [
    "print(training_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function and calculation (kld reconstruct loss) and printer form tuorial 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Help function from tutroial 6 to calculate loss and plotting\n",
    "\n",
    "def sample(mean, log_var):\n",
    "    std = torch.exp(0.5*log_var)\n",
    "    eps = torch.randn_like(std, device=mean.device)\n",
    "    return mean + eps*std\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return total_params\n",
    "def plot_losses(reco_losses, kld_losses):\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.plot(reco_losses)\n",
    "    ax.set_title('Reconstruction Loss')\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    ax.plot(kld_losses)\n",
    "    ax.set_title('KLD Loss')\n",
    "def generate_imgs(decoder_nn, cond=False):\n",
    "    num_img = 32\n",
    "    new_z = torch.randn(32, 2)\n",
    "    if torch.cuda.is_available():\n",
    "        new_z = new_z.cuda()\n",
    "\n",
    "    if cond:\n",
    "        new_labels = torch.zeros(num_img, 10).cuda()\n",
    "        rand_int = torch.randint(0, 10, (num_img,)).cuda()\n",
    "        new_labels[torch.arange(num_img), rand_int] = 1\n",
    "\n",
    "        new_pred = decoder_nn(new_z, new_labels)\n",
    "    else:\n",
    "        new_pred = decoder_nn(new_z)\n",
    "\n",
    "    new_pred = new_pred.cpu().detach().numpy()\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    gs = GridSpec(4, 8)\n",
    "\n",
    "    for i in range(32):\n",
    "        ax = fig.add_subplot(gs[i])\n",
    "        img_array = new_pred[i][0]\n",
    "        ax.imshow(img_array, cmap='gray')\n",
    "        ax.axis('off')\n",
    "        if cond:\n",
    "            ax.set_title(f'Cond input: {rand_int[i]}')\n",
    "def plot_z_dist(train_dl, encoder_nn, cond=False):\n",
    "    fig = plt.figure(figsize=(24, 6))\n",
    "    gs = GridSpec(1, 4)\n",
    "\n",
    "\n",
    "    mus = []; log_vars = []; zs = []; ys = []\n",
    "    for xb, yb, yb_one_hot in train_dl:\n",
    "        if torch.cuda.is_available():\n",
    "            xb = xb.cuda()\n",
    "            yb_one_hot = yb_one_hot.cuda()\n",
    "\n",
    "        if cond:\n",
    "            mu, log_var = encoder_nn(xb, yb_one_hot)\n",
    "        else:\n",
    "            mu, log_var = encoder_nn(xb)\n",
    "        z = sample(mu, log_var)\n",
    "\n",
    "        mu = mu.cpu().detach().numpy()\n",
    "        log_var = log_var.cpu().detach().numpy()\n",
    "        z = z.cpu().detach().numpy()\n",
    "\n",
    "        mus.extend(mu.tolist())\n",
    "        log_vars.extend(log_var.tolist())\n",
    "        zs.extend(z.tolist())\n",
    "        ys.extend(yb.tolist())\n",
    "\n",
    "    zs = np.array(zs)\n",
    "    mus = np.array(mus)\n",
    "    log_vars = np.array(log_vars)\n",
    "    ys = np.array(ys)\n",
    "\n",
    "\n",
    "    ax = fig.add_subplot(gs[0])\n",
    "    ax.scatter(zs[:, 0], zs[:, 1], c=ys, cmap='tab10', alpha=0.3, s=1)\n",
    "    ax.set_title('Z')\n",
    "    ax.grid()\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    gauss0 = np.random.normal(0, 1, len(zs[:, 0]))\n",
    "    gauss1 = np.random.normal(0, 1, len(zs[:, 0]))\n",
    "    ax = fig.add_subplot(gs[1])\n",
    "    ax.scatter(gauss0, gauss1, c='cornflowerblue', alpha=0.3, s=1)\n",
    "    ax.set_title('Gaus(0,1))')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    return ys, zs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trainning loop based on tutorial 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_VAE import EncoderLn, DecoderLn\n",
    "encoder_nn = EncoderLn()\n",
    "decoder_nn = DecoderLn()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    encoder_nn.cuda()\n",
    "    decoder_nn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(list(encoder_nn.parameters()) + list(decoder_nn.parameters()), lr=0.001)\n",
    "\n",
    "\n",
    "reco_losses = []\n",
    "kld_losses  = []\n",
    "\n",
    "for ep in tqdm(range(50)):\n",
    "    for x _ in train_dl:\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        mu, log_var = encoder_nn(xb)\n",
    "        z = sample(mu, log_var)\n",
    "\n",
    "        pred = decoder_nn(z)\n",
    "\n",
    "        # loss\n",
    "        reco_loss = BCE = F.binary_cross_entropy(pred, x, reduction='sum')\n",
    "        kld_loss  = -0.5 * torch.sum(1 + log_var - mu**2 -  log_var.exp())\n",
    "\n",
    "        loss = reco_loss + kld_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        reco_losses.append(reco_loss.item())\n",
    "        kld_losses.append(kld_loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
