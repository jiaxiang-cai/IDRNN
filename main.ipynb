{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDRNN\n",
    "[Description]\n",
    "(Add description here)\n",
    "\n",
    "The goal of the project:\n",
    "1. palceholder\n",
    "2. placeholder\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First download the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "from torch.utils.data import  Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data and print it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import ProteinDataset, proteinread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'builtin_function_or_method' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m train_data, val_data \u001b[39m=\u001b[39m train_test_split(dataset, train_size\u001b[39m=\u001b[39mtraining_size, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[39m# Create dataset instances for training and validation\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m train_data, val_data \u001b[39m=\u001b[39m ProteinDataset(train_data), ProteinDataset(val_data)\n\u001b[0;32m     17\u001b[0m \u001b[39m# Create data loaders for training and validation\u001b[39;00m\n\u001b[0;32m     18\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m  \u001b[39m# You can adjust this batch size\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pukqq\\OneDrive - weizmann.ac.il\\Practical DL\\IDRNN\\dataloader.py:46\u001b[0m, in \u001b[0;36mProteinDataset.__init__\u001b[1;34m(self, data_path)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, data_path):\n\u001b[1;32m---> 46\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m proteinread(data_path)\n",
      "File \u001b[1;32mc:\\Users\\pukqq\\OneDrive - weizmann.ac.il\\Practical DL\\IDRNN\\dataloader.py:23\u001b[0m, in \u001b[0;36mproteinread\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mproteinread\u001b[39m(filename):\n\u001b[0;32m     22\u001b[0m     \u001b[39m# Read the CSV file with sequences\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(filename, header\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m     24\u001b[0m     binary_sequence_matrices \u001b[39m=\u001b[39m []\n\u001b[0;32m     25\u001b[0m     max_sequence_length \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\common.py:707\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    704\u001b[0m errors \u001b[39m=\u001b[39m errors \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    706\u001b[0m \u001b[39m# read_csv does not know whether the buffer is opened in binary/text mode\u001b[39;00m\n\u001b[1;32m--> 707\u001b[0m \u001b[39mif\u001b[39;00m _is_binary_mode(path_or_buf, mode) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m    708\u001b[0m     mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[39m# validate encoding and errors\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\common.py:1167\u001b[0m, in \u001b[0;36m_is_binary_mode\u001b[1;34m(handle, mode)\u001b[0m\n\u001b[0;32m   1164\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mtype\u001b[39m(handle), text_classes):\n\u001b[0;32m   1165\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1167\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, _get_binary_io_classes()) \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39;49m\u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39min\u001b[39;49;00m \u001b[39mgetattr\u001b[39;49m(\n\u001b[0;32m   1168\u001b[0m     handle, \u001b[39m\"\u001b[39;49m\u001b[39mmode\u001b[39;49m\u001b[39m\"\u001b[39;49m, mode\n\u001b[0;32m   1169\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'builtin_function_or_method' is not iterable"
     ]
    }
   ],
   "source": [
    "# Replace this path with the actual path to your input CSV file\n",
    "from sklearn.model_selection import train_test_split\n",
    "input_csv_file = \"data.csv\"\n",
    "\n",
    "# Create an instance of the ProteinDataset\n",
    "dataset = proteinread(input_csv_file)\n",
    "\n",
    "# Define the validation set size (20% of the total data)\n",
    "training_size = 0.8\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_data, val_data = train_test_split(dataset, train_size=training_size, random_state=42)\n",
    "\n",
    "# Create dataset instances for training and validation\n",
    "train_data, val_data = ProteinDataset(train_data), ProteinDataset(val_data)\n",
    "\n",
    "# Create data loaders for training and validation\n",
    "batch_size = 64  # You can adjust this batch size\n",
    "train_dataset = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick up one sequence to show how our data looks like\n",
    "g = training_dataset[65]\n",
    "\n",
    "print(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get training sequence and validation sequence only\n",
    "\n",
    "\n",
    "# Create an empty list to store the numerical sequences\n",
    "training_sequences = []\n",
    "validation_sequences = []\n",
    "# Iterate over the training_dataset and  validation dataset\n",
    "for sample in training_dataset:\n",
    "    # Extract the numerical part (the first element of the tuple) and append it to the list\n",
    "    training_sequences.append(sample[0])\n",
    "\n",
    "for sample in validation_dataset:\n",
    "    # Extract the numerical part (the first element of the tuple) and append it to the list\n",
    "    validation_sequences.append(sample[0])\n",
    "    \n",
    "# Print the list of training sequences\n",
    "print(training_sequences[65])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Help function"
    ]
   },
   "outputs": [],
   "source": [
    "print(training_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function and calculation (kld reconstruct loss) and printer form tuorial 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Help function from tutroial 6 to calculate loss and plotting\n",
    "\n",
    "def sample(mean, log_var):\n",
    "    std = torch.exp(0.5*log_var)\n",
    "    eps = torch.randn_like(std, device=mean.device)\n",
    "    return mean + eps*std\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return total_params\n",
    "def plot_losses(reco_losses, kld_losses):\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.plot(reco_losses)\n",
    "    ax.set_title('Reconstruction Loss')\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    ax.plot(kld_losses)\n",
    "    ax.set_title('KLD Loss')\n",
    "def generate_imgs(decoder_nn, cond=False):\n",
    "    num_img = 32\n",
    "    new_z = torch.randn(32, 2)\n",
    "    if torch.cuda.is_available():\n",
    "        new_z = new_z.cuda()\n",
    "\n",
    "    if cond:\n",
    "        new_labels = torch.zeros(num_img, 10).cuda()\n",
    "        rand_int = torch.randint(0, 10, (num_img,)).cuda()\n",
    "        new_labels[torch.arange(num_img), rand_int] = 1\n",
    "\n",
    "        new_pred = decoder_nn(new_z, new_labels)\n",
    "    else:\n",
    "        new_pred = decoder_nn(new_z)\n",
    "\n",
    "    new_pred = new_pred.cpu().detach().numpy()\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    gs = GridSpec(4, 8)\n",
    "\n",
    "    for i in range(32):\n",
    "        ax = fig.add_subplot(gs[i])\n",
    "        img_array = new_pred[i][0]\n",
    "        ax.imshow(img_array, cmap='gray')\n",
    "        ax.axis('off')\n",
    "        if cond:\n",
    "            ax.set_title(f'Cond input: {rand_int[i]}')\n",
    "def plot_z_dist(train_dl, encoder_nn, cond=False):\n",
    "    fig = plt.figure(figsize=(24, 6))\n",
    "    gs = GridSpec(1, 4)\n",
    "\n",
    "\n",
    "    mus = []; log_vars = []; zs = []; ys = []\n",
    "    for xb, yb, yb_one_hot in train_dl:\n",
    "        if torch.cuda.is_available():\n",
    "            xb = xb.cuda()\n",
    "            yb_one_hot = yb_one_hot.cuda()\n",
    "\n",
    "        if cond:\n",
    "            mu, log_var = encoder_nn(xb, yb_one_hot)\n",
    "        else:\n",
    "            mu, log_var = encoder_nn(xb)\n",
    "        z = sample(mu, log_var)\n",
    "\n",
    "        mu = mu.cpu().detach().numpy()\n",
    "        log_var = log_var.cpu().detach().numpy()\n",
    "        z = z.cpu().detach().numpy()\n",
    "\n",
    "        mus.extend(mu.tolist())\n",
    "        log_vars.extend(log_var.tolist())\n",
    "        zs.extend(z.tolist())\n",
    "        ys.extend(yb.tolist())\n",
    "\n",
    "    zs = np.array(zs)\n",
    "    mus = np.array(mus)\n",
    "    log_vars = np.array(log_vars)\n",
    "    ys = np.array(ys)\n",
    "\n",
    "\n",
    "    ax = fig.add_subplot(gs[0])\n",
    "    ax.scatter(zs[:, 0], zs[:, 1], c=ys, cmap='tab10', alpha=0.3, s=1)\n",
    "    ax.set_title('Z')\n",
    "    ax.grid()\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    gauss0 = np.random.normal(0, 1, len(zs[:, 0]))\n",
    "    gauss1 = np.random.normal(0, 1, len(zs[:, 0]))\n",
    "    ax = fig.add_subplot(gs[1])\n",
    "    ax.scatter(gauss0, gauss1, c='cornflowerblue', alpha=0.3, s=1)\n",
    "    ax.set_title('Gaus(0,1))')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    return ys, zs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trainning loop based on tutorial 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_VAE import EncoderLn, DecoderLn\n",
    "encoder_nn = EncoderLn()\n",
    "decoder_nn = DecoderLn()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    encoder_nn.cuda()\n",
    "    decoder_nn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(list(encoder_nn.parameters()) + list(decoder_nn.parameters()), lr=0.001)\n",
    "\n",
    "\n",
    "reco_losses = []\n",
    "kld_losses  = []\n",
    "\n",
    "for ep in tqdm(range(50)):\n",
    "    for x _ in train_dl:\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        mu, log_var = encoder_nn(xb)\n",
    "        z = sample(mu, log_var)\n",
    "\n",
    "        pred = decoder_nn(z)\n",
    "\n",
    "        # loss\n",
    "        reco_loss = BCE = F.binary_cross_entropy(pred, x, reduction='sum')\n",
    "        kld_loss  = -0.5 * torch.sum(1 + log_var - mu**2 -  log_var.exp())\n",
    "\n",
    "        loss = reco_loss + kld_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        reco_losses.append(reco_loss.item())\n",
    "        kld_losses.append(kld_loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
