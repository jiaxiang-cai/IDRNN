{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDRNN\n",
    "[Description]\n",
    "(Add description here)\n",
    "\n",
    "The goal of the project:\n",
    "1. palceholder\n",
    "2. placeholder\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First download the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\rainmer\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 1)) (1.24.2)\n",
      "Requirement already satisfied: h5py in d:\\python\\lib\\site-packages (from -r requirements.txt (line 2)) (3.8.0)\n",
      "Requirement already satisfied: torch in d:\\python\\lib\\site-packages (from -r requirements.txt (line 3)) (2.0.1)\n",
      "Requirement already satisfied: tqdm in d:\\python\\lib\\site-packages (from -r requirements.txt (line 4)) (4.65.0)\n",
      "Requirement already satisfied: pytest in d:\\python\\lib\\site-packages (from -r requirements.txt (line 5)) (7.3.1)\n",
      "Requirement already satisfied: filelock in d:\\python\\lib\\site-packages (from torch->-r requirements.txt (line 3)) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in d:\\python\\lib\\site-packages (from torch->-r requirements.txt (line 3)) (4.5.0)\n",
      "Requirement already satisfied: sympy in d:\\python\\lib\\site-packages (from torch->-r requirements.txt (line 3)) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\python\\lib\\site-packages (from torch->-r requirements.txt (line 3)) (3.1)\n",
      "Requirement already satisfied: jinja2 in d:\\python\\lib\\site-packages (from torch->-r requirements.txt (line 3)) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\rainmer\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->-r requirements.txt (line 4)) (0.4.6)\n",
      "Requirement already satisfied: iniconfig in d:\\python\\lib\\site-packages (from pytest->-r requirements.txt (line 5)) (2.0.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\rainmer\\appdata\\roaming\\python\\python311\\site-packages (from pytest->-r requirements.txt (line 5)) (23.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in d:\\python\\lib\\site-packages (from pytest->-r requirements.txt (line 5)) (1.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\python\\lib\\site-packages (from jinja2->torch->-r requirements.txt (line 3)) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\python\\lib\\site-packages (from sympy->torch->-r requirements.txt (line 3)) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import h5py\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec, GridSpecFromSubplotSpec\n",
    "import idx2numpy\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import tqdm\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data and print it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the CSV file with sequences\n",
    "data = pd.read_csv(\"data.csv\", header=None)\n",
    "\n",
    "# Define a function to convert amino acids to binary vectors\n",
    "def amino_to_binary(amino):\n",
    "    amino_to_index = {'A': 0, 'C': 1, 'D': 2, 'E': 3, 'F': 4, 'G': 5, 'H': 6, 'I': 7,\n",
    "                      'K': 8, 'L': 9, 'M': 10, 'N': 11, 'P': 12, 'Q': 13, 'R': 14,\n",
    "                      'S': 15, 'T': 16, 'V': 17, 'W': 18, 'Y': 19}\n",
    "    \n",
    "    index = amino_to_index.get(amino, -1)\n",
    "    if index == -1:\n",
    "        raise ValueError(f\"Invalid amino acid: {amino}\")\n",
    "    \n",
    "    binary_vector = [0] * 20\n",
    "    binary_vector[index] = 1\n",
    "    return binary_vector\n",
    "  # Convert sequences to binary matrices\n",
    "binary_sequence_matrices = []\n",
    "max_sequence_length = 0\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    sequence = row[0]\n",
    "    binary_sequence_matrix = [amino_to_binary(amino) for amino in sequence]\n",
    "    binary_sequence_matrices.append(binary_sequence_matrix)\n",
    "    \n",
    "    if len(binary_sequence_matrix) > max_sequence_length:\n",
    "        max_sequence_length = len(binary_sequence_matrix)\n",
    "\n",
    "# Pad sequences to the same length\n",
    "padded_binary_sequence_matrices = [seq + [[0] * 20] * (max_sequence_length - len(seq)) for seq in binary_sequence_matrices]\n",
    "\n",
    "# Convert the padded matrices to a tensor\n",
    "binary_sequence_tensor = torch.tensor(padded_binary_sequence_matrices, dtype=torch.float32)\n",
    "# Split the data into a training set and a validation set\n",
    "train_size = 0.8  # You can adjust this ratio\n",
    "train_data, val_data = train_test_split(binary_sequence_tensor, train_size=train_size, random_state=42)\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Create dataset instances for training and validation\n",
    "train_dataset = CustomDataset(train_data)\n",
    "val_dataset = CustomDataset(val_data)\n",
    "\n",
    "# Create data loaders for training and validation\n",
    "batch_size = 64  # You can adjust this batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([981, 20])\n"
     ]
    }
   ],
   "source": [
    "# pick up one sequence to show how our data looks like\n",
    "g = train_dataset[33]\n",
    "\n",
    "print(g)\n",
    "print(g.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functionform tuorial 6 and IUPred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Help function from tutroial 6 to calculate loss and plotting\n",
    "\n",
    "def sample(mean, log_var):\n",
    "    std = torch.exp(0.5*log_var)\n",
    "    eps = torch.randn_like(std, device=mean.device)\n",
    "    return mean + eps*std\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return total_params\n",
    "def plot_losses(reco_losses, kld_losses):\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.plot(reco_losses)\n",
    "    ax.set_title('Reconstruction Loss')\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    ax.plot(kld_losses)\n",
    "    ax.set_title('KLD Loss')\n",
    "def generate_imgs(decoder_nn, cond=False):\n",
    "    num_img = 32\n",
    "    new_z = torch.randn(32, 2)\n",
    "    if torch.cuda.is_available():\n",
    "        new_z = new_z.cuda()\n",
    "\n",
    "    if cond:\n",
    "        new_labels = torch.zeros(num_img, 10).cuda()\n",
    "        rand_int = torch.randint(0, 10, (num_img,)).cuda()\n",
    "        new_labels[torch.arange(num_img), rand_int] = 1\n",
    "\n",
    "        new_pred = decoder_nn(new_z, new_labels)\n",
    "    else:\n",
    "        new_pred = decoder_nn(new_z)\n",
    "\n",
    "    new_pred = new_pred.cpu().detach().numpy()\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    gs = GridSpec(4, 8)\n",
    "\n",
    "    for i in range(32):\n",
    "        ax = fig.add_subplot(gs[i])\n",
    "        img_array = new_pred[i][0]\n",
    "        ax.imshow(img_array, cmap='gray')\n",
    "        ax.axis('off')\n",
    "        if cond:\n",
    "            ax.set_title(f'Cond input: {rand_int[i]}')\n",
    "def plot_z_dist(train_dl, encoder_nn, cond=False):\n",
    "    fig = plt.figure(figsize=(24, 6))\n",
    "    gs = GridSpec(1, 4)\n",
    "\n",
    "\n",
    "    mus = []; log_vars = []; zs = []; ys = []\n",
    "    for xb, yb, yb_one_hot in train_dl:\n",
    "        if torch.cuda.is_available():\n",
    "            xb = xb.cuda()\n",
    "            yb_one_hot = yb_one_hot.cuda()\n",
    "\n",
    "        if cond:\n",
    "            mu, log_var = encoder_nn(xb, yb_one_hot)\n",
    "        else:\n",
    "            mu, log_var = encoder_nn(xb)\n",
    "        z = sample(mu, log_var)\n",
    "\n",
    "        mu = mu.cpu().detach().numpy()\n",
    "        log_var = log_var.cpu().detach().numpy()\n",
    "        z = z.cpu().detach().numpy()\n",
    "\n",
    "        mus.extend(mu.tolist())\n",
    "        log_vars.extend(log_var.tolist())\n",
    "        zs.extend(z.tolist())\n",
    "        ys.extend(yb.tolist())\n",
    "\n",
    "    zs = np.array(zs)\n",
    "    mus = np.array(mus)\n",
    "    log_vars = np.array(log_vars)\n",
    "    ys = np.array(ys)\n",
    "\n",
    "\n",
    "    ax = fig.add_subplot(gs[0])\n",
    "    ax.scatter(zs[:, 0], zs[:, 1], c=ys, cmap='tab10', alpha=0.3, s=1)\n",
    "    ax.set_title('Z')\n",
    "    ax.grid()\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    gauss0 = np.random.normal(0, 1, len(zs[:, 0]))\n",
    "    gauss1 = np.random.normal(0, 1, len(zs[:, 0]))\n",
    "    ax = fig.add_subplot(gs[1])\n",
    "    ax.scatter(gauss0, gauss1, c='cornflowerblue', alpha=0.3, s=1)\n",
    "    ax.set_title('Gaus(0,1))')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    return ys, zs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trainning loop based on tutorial 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the layer dimensions for encoder and decoder\n",
    "encoder_layers = 981*20  # Input dimension, hidden layer size, and latent dimension\n",
    "decoder_layers = 64    # Latent dimension, class dimension, and output dimension\n",
    "\n",
    "# Define other parameters\n",
    "hid_layer = 5  # Number of hidden layers\n",
    "last_layer_activation = False  # Set to True if you want the last layer to have an activation function\n",
    "latent_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodel_VAE\u001b[39;00m \u001b[39mimport\u001b[39;00m EncoderLn, DecoderLn\n\u001b[1;32m----> 2\u001b[0m encoder_nn \u001b[39m=\u001b[39m EncoderLn(encoder_layers, hid_layer, latent_dim, last_layer_activation)\n\u001b[0;32m      3\u001b[0m decoder_nn \u001b[39m=\u001b[39m DecoderLn(decoder_layers, hid_layer, output_dim)\n\u001b[0;32m      5\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n",
      "File \u001b[1;32md:\\machineL_hw\\IDRNN\\model_VAE.py:20\u001b[0m, in \u001b[0;36mEncoderLn.__init__\u001b[1;34m(self, input_dim, hid_layer, latent_dim, last_layer_activation)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_dim \u001b[39m=\u001b[39m input_dim\n\u001b[0;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlatent_dim \u001b[39m=\u001b[39m latent_dim\n\u001b[0;32m     19\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menc \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m---> 20\u001b[0m     nn\u001b[39m.\u001b[39mLinear(input_dim, hid_layer[\u001b[39m0\u001b[39;49m]), nn\u001b[39m.\u001b[39mReLU()\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m hidden_i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(hid_layer) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m     23\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menc\u001b[39m.\u001b[39mappend(nn\u001b[39m.\u001b[39mLinear(hid_layer[hidden_i], hid_layer[hidden_i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m]))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from model_VAE import EncoderLn, DecoderLn\n",
    "encoder_nn = EncoderLn(encoder_layers, hid_layer, latent_dim, last_layer_activation)\n",
    "decoder_nn = DecoderLn(decoder_layers, hid_layer, output_dim)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    encoder_nn.cuda()\n",
    "    decoder_nn.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now i try to use NN form tutorial to see the mistake "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.seq1 = nn.Sequential(\n",
    "            nn.Linear(981*20, 300*20),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Linear(300*20, 200),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Linear(200, 100),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.mean_nn    = nn.Linear(100, 2)\n",
    "        self.log_var_nn = nn.Linear(100, 2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 981*20)\n",
    "        x = self.seq1(x)\n",
    "\n",
    "        mean = self.mean_nn(x)\n",
    "        log_var = self.log_var_nn(x)\n",
    "\n",
    "        return mean, log_var\n",
    "    \n",
    "\n",
    "# Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.seq1 = nn.Sequential(\n",
    "            nn.Linear(2, 100),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Linear(100, 200),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Linear(200, 300*20),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Linear(300*20, 981*20),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.seq1(x)\n",
    "        x = x.view(-1, 1, 981, 20)        \n",
    "\n",
    "        return x\n",
    "encoder_nn = Encoder()\n",
    "decoder_nn = Decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ccf2664ed794aa59655e942541878a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m kld_losses  \u001b[39m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m ep \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m50\u001b[39m)):\n\u001b[1;32m----> 6\u001b[0m     \u001b[39mfor\u001b[39;00m xb, yb \u001b[39min\u001b[39;00m train_dataset:\n\u001b[0;32m      7\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m      8\u001b[0m             xb \u001b[39m=\u001b[39m xb\u001b[39m.\u001b[39mcuda()\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(list(encoder_nn.parameters()) + list(decoder_nn.parameters()), lr=0.001)\n",
    "reco_losses = []\n",
    "kld_losses  = []\n",
    "\n",
    "for ep in tqdm(range(50)):\n",
    "    for xb, yb in train_dataset:\n",
    "        if torch.cuda.is_available():\n",
    "            xb = xb.cuda()\n",
    "            yb = yb.cuda()\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        mu, log_var = encoder_nn(xb)\n",
    "        z = sample(mu, log_var)\n",
    "\n",
    "        pred = decoder_nn(z)\n",
    "\n",
    "        # loss\n",
    "        reco_loss = BCE = F.binary_cross_entropy(pred, xb, reduction='sum')\n",
    "        kld_loss  = -0.5 * torch.sum(1 + log_var - mu**2 -  log_var.exp())\n",
    "\n",
    "        loss = reco_loss + kld_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        reco_losses.append(reco_loss.item())\n",
    "        kld_losses.append(kld_loss.item())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
